{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 지난주 개념 복습!\n",
        "아래 마크다운을 풀고, 지난주 예습과제를 통해 공부한 Vision Transformer의 특징과 다른 모델들과의 차이점을 간략히 설명해주세요."
      ],
      "metadata": {
        "id": "KTJ_ipcIRCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vision Transformer는 vision task 에 nlp transformer의 self-attention을 결한 구조로, 새로운 접근 방식을 제시하였다.\n",
        "- Vision Transformer는 이미지를 일정 크기의 패치로 자르고 각 패치를 NLP의 토큰처럼 처리한다.\n",
        "- CNN 기반이 지역적 Convolution 필터를 사용했던 것과 달리, Vision Transformer는 전역적 attention을 기반으로 한다.\n",
        "- 데이터가 대규모일 때 적합한 방식이다."
      ],
      "metadata": {
        "id": "hzgR6HMqU8GL"
      }
    },
    {
      "metadata": {
        "id": "2X1nX0HO8Pzo"
      },
      "cell_type": "markdown",
      "source": [
        "## 파이토치로 구현한 Vision Transformer\n",
        "이번 복습과제에서는 카사바 잎 질병 데이터셋으로 vision transformer를 학습시키겠습니다.\n",
        "- 첨부된 드라이브에서 필요한 파일들을 다운받을 수 있습니다."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "VvuOjOVx8Pzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225abde6-0da2-41a1-958e-ba1fafd150e4"
      },
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version 1.7\n",
        "!pip install timm\n",
        "!pip install torch_xla"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    14  100    14    0     0     69      0 --:--:-- --:--:-- --:--:--    69\n",
            "  File \"/content/pytorch-xla-env-setup.py\", line 1\n",
            "    404: Not Found\n",
            "    ^^^\n",
            "SyntaxError: illegal target for annotation\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cpu)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.15\n",
            "Requirement already satisfied: torch_xla in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기"
      ],
      "metadata": {
        "id": "mkzDbeyLJiMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xZ5JSsRJ-jLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a322b8-49cc-481c-eed3-9274cd12e2e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #.zip 파일 unzip\n",
        "# #본인 드라이브의 경로에 맞게 수정해주세요!\n",
        "# !unzip /content/drive/MyDrive/dataset/EURON_week9/train_images.zip -d  /content/drive/MyDrive/dataset/EURON_week9/train_images"
      ],
      "metadata": {
        "id": "KTSPp_3DFtT8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #본인 드라이브의 경로에 맞게 수정해주세요!\n",
        "# !unzip /content/drive/MyDrive/dataset/EURON_week9/jx_vit_base_p16_224-80ecf9dd.pth.zip -d /content/drive/MyDrive/dataset/EURON_week9/"
      ],
      "metadata": {
        "id": "DFWWlF7qHtLM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#파일 경로\n",
        "DATA_PATH = \"/content/drive/MyDrive/dataset/EURON_week9\"\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/dataset/EURON_week9/train_images\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/dataset/EURON_week9/test.jpg\"\n",
        "MODEL_PATH = (\n",
        "    \"/content/drive/MyDrive/dataset/EURON_week9/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "-RDZl6MkGG8Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리"
      ],
      "metadata": {
        "id": "Ku7W6KWzKDD4"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pXIxH6bV8Pzp"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "import timm\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import model_selection, metrics"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7FVK6SER8Pzp"
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
        "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Eg-fSDgT8Pzq"
      },
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproductibility of results\n",
        "\n",
        "    Arguments:\n",
        "        seed {int} -- Number of the seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything(1001)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파라미터"
      ],
      "metadata": {
        "id": "FtIGnNOrKHpD"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6-dhvr7d8Pzq"
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224  #ViT에 적합한 이미지 크기로 설정해주세요.\n",
        "BATCH_SIZE = 16\n",
        "LR = 2e-05\n",
        "GAMMA = 0.7\n",
        "N_EPOCHS = 1  #모델 학습을 위해서는 더 많은 에포크가 필요하지만, 파일 크기가 큰 관계로 이번 과제에서는 전 과정을 한 번만 수행하겠습니다."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mmTbSP278Pzr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e6eeaa91-af7d-4708-a104-c4aaeca58a05"
      },
      "cell_type": "code",
      "source": [
        "# pandas로 csv 파일을 읽은 뒤, 첫 5행을 출력하세요.\n",
        "df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         image_id  label\n",
              "0  1000015157.jpg      0\n",
              "1  1000201771.jpg      3\n",
              "2   100042118.jpg      1\n",
              "3  1000723321.jpg      1\n",
              "4  1000812911.jpg      3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5446f8c5-77fb-4532-a9b1-1d8b56fac3b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000015157.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000201771.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100042118.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000723321.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000812911.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5446f8c5-77fb-4532-a9b1-1d8b56fac3b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5446f8c5-77fb-4532-a9b1-1d8b56fac3b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5446f8c5-77fb-4532-a9b1-1d8b56fac3b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3b83d03a-e161-4ee5-afd6-3ba969d8e12f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b83d03a-e161-4ee5-afd6-3ba969d8e12f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3b83d03a-e161-4ee5-afd6-3ba969d8e12f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 21397,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21397,\n        \"samples\": [\n          \"2615227158.jpg\",\n          \"1277648239.jpg\",\n          \"2305895487.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0RY7_DJ48Pzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcc1565-7224-4b1d-e066-76bd8ef8c7f8"
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21397 entries, 0 to 21396\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   image_id  21397 non-null  object\n",
            " 1   label     21397 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 334.5+ KB\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_qkEJrdC8Pzs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "948b5410-43e5-4193-fe86-f1e3d67b175b"
      },
      "cell_type": "code",
      "source": [
        "df.label.value_counts().plot(kind=\"bar\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='label'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GnBvbpb38Pzs"
      },
      "cell_type": "code",
      "source": [
        "# train과 test를 9:1로 나눠주세요.\n",
        "# random_state은 자유롭게 지정 가능합니다.\n",
        "train_df, valid_df = model_selection.train_test_split(\n",
        "    df, test_size=0.1, random_state=42, stratify=df.label.values\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-dBGym2r8Pzs"
      },
      "cell_type": "code",
      "source": [
        "class CassavaDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n",
        "        super().__init__()\n",
        "        self.df_data = df.values\n",
        "        self.data_path = data_path\n",
        "        self.transforms = transforms\n",
        "        self.mode = mode\n",
        "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name, label = self.df_data[index]\n",
        "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(img)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R4_JLjHx8Pzs"
      },
      "cell_type": "code",
      "source": [
        "#데이터 증강\n",
        "transforms_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomResizedCrop(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transforms_valid = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ovuryvnO8Pzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f040b63-f5f5-4868-9e66-08a64d5cea45"
      },
      "cell_type": "code",
      "source": [
        "#사용 가능한 ViT 모델 목록\n",
        "##우리는 이중 vit_base_patch16_224를 사용합니다!\n",
        "print(\"Available Vision Transformer Models: \")\n",
        "timm.list_models(\"vit*\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Vision Transformer Models: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vit_base_mci_224',\n",
              " 'vit_base_patch8_224',\n",
              " 'vit_base_patch14_dinov2',\n",
              " 'vit_base_patch14_reg4_dinov2',\n",
              " 'vit_base_patch16_18x2_224',\n",
              " 'vit_base_patch16_224',\n",
              " 'vit_base_patch16_224_miil',\n",
              " 'vit_base_patch16_384',\n",
              " 'vit_base_patch16_clip_224',\n",
              " 'vit_base_patch16_clip_384',\n",
              " 'vit_base_patch16_clip_quickgelu_224',\n",
              " 'vit_base_patch16_gap_224',\n",
              " 'vit_base_patch16_plus_240',\n",
              " 'vit_base_patch16_plus_clip_240',\n",
              " 'vit_base_patch16_reg4_gap_256',\n",
              " 'vit_base_patch16_rope_reg1_gap_256',\n",
              " 'vit_base_patch16_rpn_224',\n",
              " 'vit_base_patch16_siglip_224',\n",
              " 'vit_base_patch16_siglip_256',\n",
              " 'vit_base_patch16_siglip_384',\n",
              " 'vit_base_patch16_siglip_512',\n",
              " 'vit_base_patch16_siglip_gap_224',\n",
              " 'vit_base_patch16_siglip_gap_256',\n",
              " 'vit_base_patch16_siglip_gap_384',\n",
              " 'vit_base_patch16_siglip_gap_512',\n",
              " 'vit_base_patch16_xp_224',\n",
              " 'vit_base_patch32_224',\n",
              " 'vit_base_patch32_384',\n",
              " 'vit_base_patch32_clip_224',\n",
              " 'vit_base_patch32_clip_256',\n",
              " 'vit_base_patch32_clip_384',\n",
              " 'vit_base_patch32_clip_448',\n",
              " 'vit_base_patch32_clip_quickgelu_224',\n",
              " 'vit_base_patch32_plus_256',\n",
              " 'vit_base_patch32_siglip_256',\n",
              " 'vit_base_patch32_siglip_gap_256',\n",
              " 'vit_base_r26_s32_224',\n",
              " 'vit_base_r50_s16_224',\n",
              " 'vit_base_r50_s16_384',\n",
              " 'vit_base_resnet26d_224',\n",
              " 'vit_base_resnet50d_224',\n",
              " 'vit_betwixt_patch16_gap_256',\n",
              " 'vit_betwixt_patch16_reg1_gap_256',\n",
              " 'vit_betwixt_patch16_reg4_gap_256',\n",
              " 'vit_betwixt_patch16_reg4_gap_384',\n",
              " 'vit_betwixt_patch16_rope_reg4_gap_256',\n",
              " 'vit_betwixt_patch32_clip_224',\n",
              " 'vit_giant_patch14_224',\n",
              " 'vit_giant_patch14_clip_224',\n",
              " 'vit_giant_patch14_dinov2',\n",
              " 'vit_giant_patch14_reg4_dinov2',\n",
              " 'vit_giant_patch16_gap_224',\n",
              " 'vit_giantopt_patch16_siglip_256',\n",
              " 'vit_giantopt_patch16_siglip_384',\n",
              " 'vit_giantopt_patch16_siglip_gap_256',\n",
              " 'vit_giantopt_patch16_siglip_gap_384',\n",
              " 'vit_gigantic_patch14_224',\n",
              " 'vit_gigantic_patch14_clip_224',\n",
              " 'vit_gigantic_patch14_clip_quickgelu_224',\n",
              " 'vit_huge_patch14_224',\n",
              " 'vit_huge_patch14_clip_224',\n",
              " 'vit_huge_patch14_clip_336',\n",
              " 'vit_huge_patch14_clip_378',\n",
              " 'vit_huge_patch14_clip_quickgelu_224',\n",
              " 'vit_huge_patch14_clip_quickgelu_378',\n",
              " 'vit_huge_patch14_gap_224',\n",
              " 'vit_huge_patch14_xp_224',\n",
              " 'vit_huge_patch16_gap_448',\n",
              " 'vit_intern300m_patch14_448',\n",
              " 'vit_large_patch14_224',\n",
              " 'vit_large_patch14_clip_224',\n",
              " 'vit_large_patch14_clip_336',\n",
              " 'vit_large_patch14_clip_quickgelu_224',\n",
              " 'vit_large_patch14_clip_quickgelu_336',\n",
              " 'vit_large_patch14_dinov2',\n",
              " 'vit_large_patch14_reg4_dinov2',\n",
              " 'vit_large_patch14_xp_224',\n",
              " 'vit_large_patch16_224',\n",
              " 'vit_large_patch16_384',\n",
              " 'vit_large_patch16_siglip_256',\n",
              " 'vit_large_patch16_siglip_384',\n",
              " 'vit_large_patch16_siglip_512',\n",
              " 'vit_large_patch16_siglip_gap_256',\n",
              " 'vit_large_patch16_siglip_gap_384',\n",
              " 'vit_large_patch16_siglip_gap_512',\n",
              " 'vit_large_patch32_224',\n",
              " 'vit_large_patch32_384',\n",
              " 'vit_large_r50_s32_224',\n",
              " 'vit_large_r50_s32_384',\n",
              " 'vit_little_patch16_reg1_gap_256',\n",
              " 'vit_little_patch16_reg4_gap_256',\n",
              " 'vit_medium_patch16_clip_224',\n",
              " 'vit_medium_patch16_gap_240',\n",
              " 'vit_medium_patch16_gap_256',\n",
              " 'vit_medium_patch16_gap_384',\n",
              " 'vit_medium_patch16_reg1_gap_256',\n",
              " 'vit_medium_patch16_reg4_gap_256',\n",
              " 'vit_medium_patch16_rope_reg1_gap_256',\n",
              " 'vit_medium_patch32_clip_224',\n",
              " 'vit_mediumd_patch16_reg4_gap_256',\n",
              " 'vit_mediumd_patch16_reg4_gap_384',\n",
              " 'vit_mediumd_patch16_rope_reg1_gap_256',\n",
              " 'vit_pwee_patch16_reg1_gap_256',\n",
              " 'vit_relpos_base_patch16_224',\n",
              " 'vit_relpos_base_patch16_cls_224',\n",
              " 'vit_relpos_base_patch16_clsgap_224',\n",
              " 'vit_relpos_base_patch16_plus_240',\n",
              " 'vit_relpos_base_patch16_rpn_224',\n",
              " 'vit_relpos_base_patch32_plus_rpn_256',\n",
              " 'vit_relpos_medium_patch16_224',\n",
              " 'vit_relpos_medium_patch16_cls_224',\n",
              " 'vit_relpos_medium_patch16_rpn_224',\n",
              " 'vit_relpos_small_patch16_224',\n",
              " 'vit_relpos_small_patch16_rpn_224',\n",
              " 'vit_small_patch8_224',\n",
              " 'vit_small_patch14_dinov2',\n",
              " 'vit_small_patch14_reg4_dinov2',\n",
              " 'vit_small_patch16_18x2_224',\n",
              " 'vit_small_patch16_36x1_224',\n",
              " 'vit_small_patch16_224',\n",
              " 'vit_small_patch16_384',\n",
              " 'vit_small_patch32_224',\n",
              " 'vit_small_patch32_384',\n",
              " 'vit_small_r26_s32_224',\n",
              " 'vit_small_r26_s32_384',\n",
              " 'vit_small_resnet26d_224',\n",
              " 'vit_small_resnet50d_s16_224',\n",
              " 'vit_so150m2_patch16_reg1_gap_256',\n",
              " 'vit_so150m2_patch16_reg1_gap_384',\n",
              " 'vit_so150m2_patch16_reg1_gap_448',\n",
              " 'vit_so150m_patch16_reg4_gap_256',\n",
              " 'vit_so150m_patch16_reg4_gap_384',\n",
              " 'vit_so150m_patch16_reg4_map_256',\n",
              " 'vit_so400m_patch14_siglip_224',\n",
              " 'vit_so400m_patch14_siglip_378',\n",
              " 'vit_so400m_patch14_siglip_384',\n",
              " 'vit_so400m_patch14_siglip_gap_224',\n",
              " 'vit_so400m_patch14_siglip_gap_378',\n",
              " 'vit_so400m_patch14_siglip_gap_384',\n",
              " 'vit_so400m_patch14_siglip_gap_448',\n",
              " 'vit_so400m_patch14_siglip_gap_896',\n",
              " 'vit_so400m_patch16_siglip_256',\n",
              " 'vit_so400m_patch16_siglip_384',\n",
              " 'vit_so400m_patch16_siglip_512',\n",
              " 'vit_so400m_patch16_siglip_gap_256',\n",
              " 'vit_so400m_patch16_siglip_gap_384',\n",
              " 'vit_so400m_patch16_siglip_gap_512',\n",
              " 'vit_srelpos_medium_patch16_224',\n",
              " 'vit_srelpos_small_patch16_224',\n",
              " 'vit_tiny_patch16_224',\n",
              " 'vit_tiny_patch16_384',\n",
              " 'vit_tiny_r_s16_p8_224',\n",
              " 'vit_tiny_r_s16_p8_384',\n",
              " 'vit_wee_patch16_reg1_gap_256',\n",
              " 'vit_xsmall_patch16_clip_224',\n",
              " 'vitamin_base_224',\n",
              " 'vitamin_large2_224',\n",
              " 'vitamin_large2_256',\n",
              " 'vitamin_large2_336',\n",
              " 'vitamin_large2_384',\n",
              " 'vitamin_large_224',\n",
              " 'vitamin_large_256',\n",
              " 'vitamin_large_336',\n",
              " 'vitamin_large_384',\n",
              " 'vitamin_small_224',\n",
              " 'vitamin_xlarge_256',\n",
              " 'vitamin_xlarge_336',\n",
              " 'vitamin_xlarge_384']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 설계\n",
        "- 클래스 ViTBase16의 각 줄을 설명하는 주석을 달아주세요. (최소 15개)"
      ],
      "metadata": {
        "id": "UfLeB5QwLWR0"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VXdkxB5L8Pzt"
      },
      "cell_type": "code",
      "source": [
        "# 이 셀 안의 코드에 대해서 주석을 달아주세요.\n",
        "class ViTBase16(nn.Module):\n",
        "    def __init__(self, n_classes, pretrained=False):\n",
        "\n",
        "        super(ViTBase16, self).__init__()\n",
        "\n",
        "        # 사전학습 ViT 로드\n",
        "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)  # pretrained: 사전 학습 가중치 사용여부\n",
        "        if pretrained:  # pretrained=True 면 외부 저장 모델 가중치 불러와 초기화\n",
        "            self.model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "        # 새로운 데이터셋의 클래스 수에 맞게 head(마지막 분류층) 수 조정\n",
        "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
        "\n",
        "    # 순전파 정의\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    # 1 에포크 학습 과정 정의\n",
        "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
        "        epoch_loss = 0.0  # 손실 초기화\n",
        "        epoch_accuracy = 0.0  # 정확도 초기화\n",
        "\n",
        "        self.model.train()\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            if device.type == \"cuda\":  # GPU 환경일 경우\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            elif device.type == \"xla\":  # TPU 환경일 경우\n",
        "                data = data.to(device, dtype=torch.float32)\n",
        "                target = target.to(device, dtype=torch.int64)\n",
        "\n",
        "            optimizer.zero_grad()  # 기울기 초기화\n",
        "            output = self.forward(data)  # 모델 예측\n",
        "            loss = criterion(output, target)  # loss 계산\n",
        "            loss.backward()  # 역전파\n",
        "            accuracy = (output.argmax(dim=1) == target).float().mean()  # accuracy 계산\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_accuracy += accuracy\n",
        "\n",
        "            if device.type == \"xla\":\n",
        "                xm.optimizer_step(optimizer)\n",
        "\n",
        "                if i % 20 == 0:\n",
        "                    xm.master_print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n",
        "\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
        "\n",
        "    # 1 에포크 검증 과정 정의\n",
        "    def validate_one_epoch(self, valid_loader, criterion, device):\n",
        "        valid_loss = 0.0\n",
        "        valid_accuracy = 0.0\n",
        "\n",
        "        self.model.eval()\n",
        "        for data, target in valid_loader:\n",
        "            if device.type == \"cuda\":\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            elif device.type == \"xla\":\n",
        "                data = data.to(device, dtype=torch.float32)\n",
        "                target = target.to(device, dtype=torch.int64)\n",
        "\n",
        "            with torch.no_grad():  # grad 계산 X\n",
        "                output = self.model(data)\n",
        "                loss = criterion(output, target)\n",
        "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
        "\n",
        "                valid_loss += loss\n",
        "                valid_accuracy += accuracy\n",
        "\n",
        "        # 평균 loss, 정확도 반환\n",
        "        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "h63seuBU8Pzt"
      },
      "cell_type": "code",
      "source": [
        "def fit_tpu(\n",
        "    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n",
        "):\n",
        "\n",
        "    valid_loss_min = np.inf\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_accs = []\n",
        "    valid_accs = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        gc.collect()\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
        "\n",
        "        xm.master_print(f\"{'='*50}\")\n",
        "        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n",
        "        train_loss, train_acc = model.train_one_epoch(\n",
        "            para_train_loader.per_device_loader(device), criterion, optimizer, device\n",
        "        )\n",
        "        xm.master_print(\n",
        "            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        gc.collect()\n",
        "\n",
        "        if valid_loader is not None:\n",
        "            gc.collect()\n",
        "            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n",
        "            valid_loss, valid_acc = model.validate_one_epoch(\n",
        "                para_valid_loader.per_device_loader(device), criterion, device\n",
        "            )\n",
        "            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n",
        "            valid_losses.append(valid_loss)\n",
        "            valid_accs.append(valid_acc)\n",
        "            gc.collect()\n",
        "\n",
        "            if valid_loss <= valid_loss_min and epoch != 1:\n",
        "                xm.master_print(\n",
        "                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n",
        "                        valid_loss_min, valid_loss\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"valid_acc\": valid_accs,\n",
        "    }"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GV_5ivmr8Pzu"
      },
      "cell_type": "code",
      "source": [
        "model = ViTBase16(n_classes=5, pretrained=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-y_dQgzz8Pzu"
      },
      "cell_type": "code",
      "source": [
        "def _run():\n",
        "    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n",
        "    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        valid_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  #cross entropy loss로 설정.\n",
        "    device = xm.xla_device()\n",
        "    model.to(device)\n",
        "\n",
        "    lr = LR * xm.xrt_world_size()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n",
        "    start_time = datetime.now()\n",
        "    xm.master_print(f\"Start Time: {start_time}\")\n",
        "\n",
        "    logs = fit_tpu(\n",
        "        model=model,\n",
        "        epochs=N_EPOCHS,\n",
        "        device=device,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "    )\n",
        "\n",
        "    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n",
        "\n",
        "    xm.master_print(\"Saving Model\")\n",
        "    xm.save(\n",
        "        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Tbu-8P_e8Pzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61743f55-1171-4e48-b0f0-56636fd2e670"
      },
      "cell_type": "code",
      "source": [
        "# Training 시작\n",
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "    a = _run()\n",
        "\n",
        "\n",
        "# Run\n",
        "FLAGS = {}\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method=\"fork\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIALIZING TRAINING ON 1 TPU CORES\n",
            "Start Time: 2025-05-12 05:07:40.732935\n",
            "==================================================\n",
            "EPOCH 1 - TRAINING...\n",
            "\n",
            "\t[TRAIN] EPOCH 1 - LOSS: 0.0, ACCURACY: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-9 (_loader_worker):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "    _, data = next(data_iter)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "    return self._process_data(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "    raise exception\n",
            "FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-16-38a134e4a4ed>\", line 17, in __getitem__\n",
            "    img = Image.open(img_path).convert(\"RGB\")\n",
            "          ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/EURON_week9/train_images/3917910837.jpg'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1 - VALIDATING...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-11 (_loader_worker):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "    _, data = next(data_iter)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "    return self._process_data(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "    raise exception\n",
            "FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-16-38a134e4a4ed>\", line 17, in __getitem__\n",
            "    img = Image.open(img_path).convert(\"RGB\")\n",
            "          ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/EURON_week9/train_images/3512385051.jpg'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[VALID] LOSS: 0.0, ACCURACY: 0.0\n",
            "\n",
            "Execution time: 0:00:41.987405\n",
            "Saving Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSkfEJXt47Ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}