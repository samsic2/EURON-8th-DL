{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07326589-506e-46ad-9a07-b8dd7d90c732",
      "metadata": {
        "id": "07326589-506e-46ad-9a07-b8dd7d90c732"
      },
      "source": [
        "## Skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Korpora\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "xD9kSvoV1yVv",
        "outputId": "282a979c-2fc9-449f-e907-d36c2f787d27"
      },
      "id": "xD9kSvoV1yVv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting dataclasses>=0.6 (from Korpora)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.4.26)\n",
            "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, Korpora\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "83d469e3bef5493c86efd08365b2ea7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "413033d3-3d9c-4d9d-863e-7450989f4bc1",
      "metadata": {
        "id": "413033d3-3d9c-4d9d-863e-7450989f4bc1"
      },
      "outputs": [],
      "source": [
        "# ### 임베딩 클래스\n",
        "# embedding = torch.nnEmbedding(\n",
        "#     num_embeddings,\n",
        "#     embedding_dim,\n",
        "#     padding_idx=None,\n",
        "#     max_norm=None,\n",
        "#     norm_type=2.0\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4ae32199-fa6f-4263-8b49-0aca1d7366db",
      "metadata": {
        "id": "4ae32199-fa6f-4263-8b49-0aca1d7366db"
      },
      "outputs": [],
      "source": [
        "### 6.3 기본 skip-gram 클래스\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523db65d-dab0-4fce-9b9c-11c0f0288cc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "523db65d-dab0-4fce-9b9c-11c0f0288cc1",
        "outputId": "2e98d90b-6269-4781-e9c4-4ecf4fc83791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 51.3MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 44.4MB/s]                            \n"
          ]
        }
      ],
      "source": [
        "### 6.4 영화 리뷰 데이터세트 전처리\n",
        "\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)\n",
        "\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c34534-10d2-4196-a2dd-546a4c70248a",
      "metadata": {
        "id": "28c34534-10d2-4196-a2dd-546a4c70248a"
      },
      "outputs": [],
      "source": [
        "### 6.5 단어 사전 구축\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "149da6ec-ba98-4ec0-814d-379a545afafb",
      "metadata": {
        "id": "149da6ec-ba98-4ec0-814d-379a545afafb"
      },
      "outputs": [],
      "source": [
        "### 6.6 Skip-gram의 단어 쌍 추출\n",
        "\n",
        "def get_word_pairs(tokens, window_size):\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence):\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            center_word = sentence[idx]\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff45f02-fbfc-4419-ac7c-684456f11513",
      "metadata": {
        "id": "2ff45f02-fbfc-4419-ac7c-684456f11513"
      },
      "outputs": [],
      "source": [
        "### 6.7 인덱스 쌍 변환\n",
        "\n",
        "def get_index_pairs(word_pairs, token_to_id):\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"]\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282214f7-5e31-4b91-8fff-ccd60e184375",
      "metadata": {
        "id": "282214f7-5e31-4b91-8fff-ccd60e184375"
      },
      "outputs": [],
      "source": [
        "### 6.8 데이터로더 적용\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "index_pairs = torch.tensor(index_pairs)\n",
        "center_indexes = index_pairs[:, 0]\n",
        "context_indexes = index_pairs[:, 1]\n",
        "\n",
        "dataset = TensorDataset(center_indexes, context_indexes)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4ecec2-4cac-421d-85dd-5db4d60bd213",
      "metadata": {
        "id": "ec4ecec2-4cac-421d-85dd-5db4d60bd213"
      },
      "outputs": [],
      "source": [
        "### 6.9 Skip-gram 모델 준비 작업\n",
        "\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18fd3b4-66c8-45cd-93ac-6e32fae68995",
      "metadata": {
        "id": "c18fd3b4-66c8-45cd-93ac-6e32fae68995"
      },
      "outputs": [],
      "source": [
        "### 6.10 모델 학습\n",
        "\n",
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        logits = word2vec(input_ids)\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430f997a-48df-49db-b439-79b14fa90baf",
      "metadata": {
        "id": "430f997a-48df-49db-b439-79b14fa90baf"
      },
      "outputs": [],
      "source": [
        "### 6.11 임베딩 값 추출\n",
        "\n",
        "\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[index]\n",
        "token_embedding = token_to_embedding[token]\n",
        "print(token)\n",
        "print(token_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90a40eb-c1c1-4ac9-9e13-4d469db28781",
      "metadata": {
        "id": "f90a40eb-c1c1-4ac9-9e13-4d469db28781"
      },
      "outputs": [],
      "source": [
        "### 6.12 단어 임베딩 유사도 계산\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n):\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a207e9a8-eb31-4f46-a2d5-329b11229bd8",
      "metadata": {
        "id": "a207e9a8-eb31-4f46-a2d5-329b11229bd8"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993eecc5-395a-49f8-a8f1-4d8a47c534c9",
      "metadata": {
        "id": "993eecc5-395a-49f8-a8f1-4d8a47c534c9"
      },
      "outputs": [],
      "source": [
        "# ### 젠심 라이브러리 설치\n",
        "# !pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip intsall Word2Vec"
      ],
      "metadata": {
        "id": "wG_pMM96NLTB"
      },
      "id": "wG_pMM96NLTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c40d7f-41fd-49b9-b5f8-3085d1c5b859",
      "metadata": {
        "id": "91c40d7f-41fd-49b9-b5f8-3085d1c5b859"
      },
      "outputs": [],
      "source": [
        "# ### Word2Vec 클래스\n",
        "# word2vec = gensim.models.Word2Vec(\n",
        "#     sentences=None,\n",
        "#     corpus_file=None,\n",
        "#     vector_size=100,\n",
        "#     alpha=0.025,\n",
        "#     window=5,\n",
        "#     min_count=5,\n",
        "#     workers=3,\n",
        "#     sg=0,\n",
        "#     hs=0,\n",
        "#     cbow_mean=1,\n",
        "#     negative=5,\n",
        "#     ns_exponent=0.75,\n",
        "#     max_fianl_vocab=None,\n",
        "#     epochs=5,\n",
        "#     batch_words=10000\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7eef30-a17c-4096-bd90-65fd5b3ae0f6",
      "metadata": {
        "id": "dd7eef30-a17c-4096-bd90-65fd5b3ae0f6"
      },
      "outputs": [],
      "source": [
        "### 6.13 Word2Vec 모델 학습\n",
        "\n",
        "for gensim.models import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    max_fianl_vocab=10000,\n",
        ")\n",
        "\n",
        "# word2vec.save(\"../models/word2vec.model\")\n",
        "# word2vec = Word2Vec.load(\"../models/word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2df9c9-4736-4f8a-8a7d-6c7816333b35",
      "metadata": {
        "id": "ce2df9c9-4736-4f8a-8a7d-6c7816333b35"
      },
      "outputs": [],
      "source": [
        "word = \"연기\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5))\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62a040f-264e-49b0-81f0-1ced48a9cbea",
      "metadata": {
        "id": "e62a040f-264e-49b0-81f0-1ced48a9cbea"
      },
      "source": [
        "## fastText OOV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a30cd83-2cc2-4b6a-8275-b297fb2e7e41",
      "metadata": {
        "id": "6a30cd83-2cc2-4b6a-8275-b297fb2e7e41"
      },
      "outputs": [],
      "source": [
        "# ### FastText 클래스\n",
        "# fasttext gensim.models.FastText(\n",
        "#     sentences=None,\n",
        "#     corpus_file=None,\n",
        "#     vector_size=100,\n",
        "#     alpha=0.025,\n",
        "#     window=5,\n",
        "#     min_count=5,\n",
        "#     workers=3,\n",
        "#     sg=0,\n",
        "#     hs=0,\n",
        "#     cbow_mean=1,\n",
        "#     negative=5,\n",
        "#     ns_exponent=0,75,\n",
        "#     max_final_vocab=None,\n",
        "#     epochs=5,\n",
        "#     batch_words=10000,\n",
        "#     min_n=3,\n",
        "#     max_n=6\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59aa43c2-45c6-451a-858a-649437a864f4",
      "metadata": {
        "id": "59aa43c2-45c6-451a-858a-649437a864f4"
      },
      "outputs": [],
      "source": [
        "### 6.15 KorNLI 데이터세트 전처리\n",
        "\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"kornli\")\n",
        "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "tokens = [sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889194c4-425f-463a-af48-6ca194ea5825",
      "metadata": {
        "id": "889194c4-425f-463a-af48-6ca194ea5825"
      },
      "outputs": [],
      "source": [
        "### 6.16 fastText 모델 실습\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "fastText = FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    max_final_vocab=20000,\n",
        "    epochs=3,\n",
        "    min_n=2,\n",
        "    max_n=6\n",
        ")\n",
        "\n",
        "# fastText.save(\"../models/fastText.model\")\n",
        "# fastText = FastText.load(\"../models/fastText.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80210109-564b-4f0c-9e05-07b11ae91e59",
      "metadata": {
        "id": "80210109-564b-4f0c-9e05-07b11ae91e59"
      },
      "outputs": [],
      "source": [
        "### 6.17 fastText OOV 처리\n",
        "\n",
        "oov_token = \"사랑해요\"\n",
        "oov_vector = fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key)\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be7f49e-7677-4498-bdd9-caa0ccf6931a",
      "metadata": {
        "id": "3be7f49e-7677-4498-bdd9-caa0ccf6931a"
      },
      "source": [
        "### 양방향 다층 신경망"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b16d1a-622d-44fd-b950-0527302723a4",
      "metadata": {
        "id": "a6b16d1a-622d-44fd-b950-0527302723a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "ouput_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "\n",
        "model = nn.RNN(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    nonlinearity=\"tanh\",\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "outputs, hidden = model(inputs, h_0)\n",
        "print(outputs.shape)\n",
        "print(hidden.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6f5ea3-825d-4ab8-8a98-e734d7e37198",
      "metadata": {
        "id": "8f6f5ea3-825d-4ab8-8a98-e734d7e37198"
      },
      "source": [
        "### 순환 신경망"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e0e4f5-05d5-42c5-b4a4-44dc045d5d61",
      "metadata": {
        "id": "23e0e4f5-05d5-42c5-b4a4-44dc045d5d61"
      },
      "outputs": [],
      "source": [
        "# ### 순환 신경망 클래스\n",
        "# rnn = torch.nn.RNN(\n",
        "#     input_size,\n",
        "#     hidden_size,\n",
        "#     num_layers=1,\n",
        "#     nonlinearity=\"tanh\",\n",
        "#     bias=false,\n",
        "#     batch_first=True,\n",
        "#     dropout=0,\n",
        "#     bidirectional=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da7ed903-0baf-4394-bed3-50762e939372",
      "metadata": {
        "id": "da7ed903-0baf-4394-bed3-50762e939372"
      },
      "outputs": [],
      "source": [
        "### 6.18 양방향 다층 신경망\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "output_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "proj_size = 64\n",
        "\n",
        "model = nn.LSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=output_size,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        "    proj_size=proj_size,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)\n",
        "\n",
        "outputs, hidden = model(inputs, h_0)\n",
        "print(outputs.shape)\n",
        "print(hidden.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b872d78-3649-4572-8c59-811a6b7212f1",
      "metadata": {
        "id": "9b872d78-3649-4572-8c59-811a6b7212f1"
      },
      "outputs": [],
      "source": [
        "# ### 장단기 메모리 클래스\n",
        "# lstm = torch.nn.LSTM(\n",
        "#     input_size,\n",
        "#     hidden_size,\n",
        "#     num_layers=1,\n",
        "#     bias=Flase,\n",
        "#     batch_first=True,\n",
        "#     dropout=0,\n",
        "#     bidirectional=False,\n",
        "#     proj_size=0\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702842bd-9883-49af-8d64-89cce73bdac4",
      "metadata": {
        "id": "702842bd-9883-49af-8d64-89cce73bdac4"
      },
      "outputs": [],
      "source": [
        "### 6.19 양방향 다층 장단기 메모리\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "output_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "proj_size = 64\n",
        "\n",
        "model = nn.LSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=output_size,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        "    proj_size=proj_size,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0 = torch.rand(\n",
        "    num_layers * (int(bidirectional) + 1),\n",
        "    batch_size,\n",
        "    proj_size if proj_size > 0 else output_size,\n",
        ")\n",
        "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)\n",
        "\n",
        "outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n",
        "\n",
        "print(outputs.shape)\n",
        "print(h_n.shape)\n",
        "print(c_n.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df86c1b-72ab-4091-a619-29f59c7721ce",
      "metadata": {
        "id": "3df86c1b-72ab-4091-a619-29f59c7721ce"
      },
      "source": [
        "### 문장 분류 모델 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959fac47-63d1-478c-8507-b33a085e99a5",
      "metadata": {
        "id": "959fac47-63d1-478c-8507-b33a085e99a5"
      },
      "outputs": [],
      "source": [
        "### 6.20 문장 분류 모델\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8fc128b-4958-4d05-aed0-13e4bcd4631d",
      "metadata": {
        "id": "d8fc128b-4958-4d05-aed0-13e4bcd4631d"
      },
      "outputs": [],
      "source": [
        "### 6.21 데이터세트 불러오기\n",
        "\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)\n",
        "\n",
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f41c7f3-563a-453f-bbf0-1d221bd95437",
      "metadata": {
        "id": "9f41c7f3-563a-453f-bbf0-1d221bd95437"
      },
      "outputs": [],
      "source": [
        "### 6.22 데이터 토큰화 및 단어 사전 구축\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c95c6d6-92b4-4af0-b42b-ee1609ea6c00",
      "metadata": {
        "id": "6c95c6d6-92b4-4af0-b42b-ee1609ea6c00"
      },
      "outputs": [],
      "source": [
        "### 6.23 정수 인코딩 및 패딩\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93ce975-c8b8-4c29-8257-05f52fa76767",
      "metadata": {
        "id": "e93ce975-c8b8-4c29-8257-05f52fa76767"
      },
      "outputs": [],
      "source": [
        "### 6.24 데이터로더 적용\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4e87f1-9ed1-4211-ba3a-f2135ac9b004",
      "metadata": {
        "id": "4c4e87f1-9ed1-4211-ba3a-f2135ac9b004"
      },
      "outputs": [],
      "source": [
        "### 6.25 손실 함수와 최적화 함수 정의\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "n_vocab = len(token_to_id)\n",
        "hidden_dim = 64\n",
        "embedding_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653a5fc8-0900-4871-8627-7d0f6bc42b8e",
      "metadata": {
        "id": "653a5fc8-0900-4871-8627-7d0f6bc42b8e"
      },
      "outputs": [],
      "source": [
        "### 6.26 모델 학습 및 테스트\n",
        "\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list()\n",
        "    corrects = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0096779-5349-4766-8b69-4877c2f72ac3",
      "metadata": {
        "id": "f0096779-5349-4766-8b69-4877c2f72ac3"
      },
      "outputs": [],
      "source": [
        "### 6.27 학습된 모델로부터 임베딩 추출\n",
        "\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, emb in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = emb\n",
        "\n",
        "token = vocab[1000]\n",
        "print(token, token_to_embedding[token])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be3aae7-7a51-4f06-a7a7-7d601d97ebc5",
      "metadata": {
        "id": "9be3aae7-7a51-4f06-a7a7-7d601d97ebc5"
      },
      "source": [
        "### Word2Vec 모델로 사전 학습된 임베딩 값을 초깃값으로 적용해 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f080b8e2-422f-4a97-af73-02b48ed2f3e0",
      "metadata": {
        "id": "f080b8e2-422f-4a97-af73-02b48ed2f3e0"
      },
      "outputs": [],
      "source": [
        "### 6.28 사전 학습된 모델로 임베딩 계층 초기화\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec = Word2vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index] = word2vec wvltoken]\n",
        "\n",
        "embedding_layer nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60299a34-08fc-457c-a907-14c614cee816",
      "metadata": {
        "id": "60299a34-08fc-457c-a907-14c614cee816"
      },
      "outputs": [],
      "source": [
        "### 6.29 사전 학습된 임베딩 계층 적용(SentenceClassifier의 변경점)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,                     # 여기 바뀜\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\",\n",
        "        pretrained_embedding=None  # 여기 바뀜\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        ### 여기서부터 바뀜 ###\n",
        "        if pretrained_embedding is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(\n",
        "                num_embeddings=n_vocab,\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0\n",
        "            )\n",
        "\n",
        "        ### 여기서부터 같음 ###\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d65665a-19e4-4de9-814b-ee5833bff054",
      "metadata": {
        "id": "1d65665a-19e4-4de9-814b-ee5833bff054"
      },
      "outputs": [],
      "source": [
        "### 6.30 사전 학습된 임베딩을 사용한 모델 학습\n",
        "\n",
        "\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
        "    n_layers=n_layers, pretrained_embedding=init_embeddings\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d9122e-f95a-4941-b001-2736d42e4ec1",
      "metadata": {
        "id": "b8d9122e-f95a-4941-b001-2736d42e4ec1"
      },
      "source": [
        "### 합성곱 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005335bc-f843-4394-86d0-91bd24b1af1c",
      "metadata": {
        "id": "005335bc-f843-4394-86d0-91bd24b1af1c"
      },
      "outputs": [],
      "source": [
        "# ### 2차원 합성곱 계층 클래스\n",
        "# conv = torch.nn.Conv2d(\n",
        "#     in_channels,\n",
        "#     out_channels,\n",
        "#     kernel_size,\n",
        "#     stride=1,\n",
        "#     padding=0,\n",
        "#     dilation=1,\n",
        "#     groups=1,\n",
        "#     bias=True,\n",
        "#     padding_mode=\"zeros\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0bbc3c-85bf-40cd-a313-88b47ff5a0f6",
      "metadata": {
        "id": "6b0bbc3c-85bf-40cd-a313-88b47ff5a0f6"
      },
      "outputs": [],
      "source": [
        "# ### 2차원 최댓값 풀링 클래스\n",
        "# pool = torch.nn.MaxPool2d(\n",
        "#     kernel_size,\n",
        "#     stride=None,\n",
        "#     padding=0,\n",
        "#     dilation=1\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee747bf-b477-4647-9914-48b676c7b508",
      "metadata": {
        "id": "0ee747bf-b477-4647-9914-48b676c7b508"
      },
      "outputs": [],
      "source": [
        "# ### 2차원 평균값 풀링 클래스\n",
        "# pool = torch.nn.AvgPool2d(\n",
        "#     kernel_size,\n",
        "#     stride=None,\n",
        "#     padding=0,\n",
        "#     count_include_pad=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f4fab1-fa0c-4001-baec-4ffef0c8c2f6",
      "metadata": {
        "id": "61f4fab1-fa0c-4001-baec-4ffef0c8c2f6"
      },
      "outputs": [],
      "source": [
        "### 6.31 합성곱 모델\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(32 * 32 * 32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889b0cec-a0e5-4dc9-ab79-d0bf4c595849",
      "metadata": {
        "id": "889b0cec-a0e5-4dc9-ab79-d0bf4c595849"
      },
      "outputs": [],
      "source": [
        "### 6.32 합성곱 기반 분류 모델 정의\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "        )\n",
        "        embedding_dim = self.embedding.weight.shape[1]\n",
        "\n",
        "        conv = []\n",
        "        for size in filter_sizes:\n",
        "            conv.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels=embedding_dim,\n",
        "                        out_channels=1,\n",
        "                        kernel_size=size\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
        "                )\n",
        "            )\n",
        "        self.conv_filters = nn.ModuleList(conv)\n",
        "\n",
        "        output_size = len(filter_sizes)\n",
        "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(output_size, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = embeddings.permute(0, 2, 1)\n",
        "\n",
        "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
        "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
        "\n",
        "        logits = self.pre_classifier(concat_outputs)\n",
        "        logits = self.dropout(logits)\n",
        "        logits = self.classifier(logits)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d8f8bb-ad4d-447e-a017-a4cd2e109d30",
      "metadata": {
        "id": "a1d8f8bb-ad4d-447e-a017-a4cd2e109d30"
      },
      "source": [
        "### 합성곱 신경망 분류 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91399efd-a38a-4907-bd55-6cefc451f144",
      "metadata": {
        "id": "91399efd-a38a-4907-bd55-6cefc451f144"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)\n",
        "\n",
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "n_vocab = len(token_to_id)\n",
        "embedding_dim = 128\n",
        "\n",
        "\n",
        "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index] = word2vec.wv[token]\n",
        "\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "n_vocab = len(token_to_id)\n",
        "embedding_dim = 128\n",
        "\n",
        "\n",
        "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index] = word2vec.wv[token]\n",
        "\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62f5e76-07bb-49ae-9ed0-a6b59da9e3b9",
      "metadata": {
        "id": "c62f5e76-07bb-49ae-9ed0-a6b59da9e3b9"
      },
      "outputs": [],
      "source": [
        "### 달라지는 부분 ###\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47e5057-ff7b-401a-9e30-d174a6b2f305",
      "metadata": {
        "id": "d47e5057-ff7b-401a-9e30-d174a6b2f305"
      },
      "outputs": [],
      "source": [
        "### 다시 같은 부분\n",
        "\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list()\n",
        "    corrects = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6daaada1-593a-491b-886a-cc6d60e9ff58",
      "metadata": {
        "id": "6daaada1-593a-491b-886a-cc6d60e9ff58"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}